{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def normalize_omissions(text):\n",
    "    pattern = re.compile(r\"\\w*['’]\\w*\", re.IGNORECASE)\n",
    "    tokens = text.split()\n",
    "    norm_str = text\n",
    "    for token in tokens:\n",
    "        if re.match(pattern, token):\n",
    "            m = re.split(r\"['’]\", token)\n",
    "            n = (\"a \").join(m)\n",
    "            norm_str = norm_str.replace(token, n)\n",
    "    return norm_str\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\n",
    "        'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stop_words(text, stopwords):\n",
    "    return \" \".join(w for w in word_tokenize(text) if w not in stopwords)\n",
    "\n",
    "\n",
    "def normalize_text(doc, html_stripping=True, handle_omissions=True, accented_char_removal=True, text_lower_case=True, special_chars_removal=True, remove_digits=True, stopwords_removal=False, stopwords=None):\n",
    "    # strip html\n",
    "    if html_stripping:\n",
    "        doc = strip_html_tags(doc)\n",
    "    if handle_omissions:\n",
    "        doc = normalize_omissions(doc)\n",
    "    # remove accented chars\n",
    "    if accented_char_removal:\n",
    "        doc = remove_accented_chars(doc)\n",
    "    # lowercase the text\n",
    "    if text_lower_case:\n",
    "        doc = doc.lower()\n",
    "    # remove extra newlines\n",
    "    doc = re.sub(r'[\\r|\\n|\\r\\n\\t]+', ' ', doc)\n",
    "    # remove special characters and digits\n",
    "    if special_chars_removal:\n",
    "        special_chars_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_chars_pattern.sub(\" \\\\1 \", doc)\n",
    "        doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "    # remove extra whitespace\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    # remove stopwords\n",
    "    if stopwords_removal and stopwords:\n",
    "        doc = remove_stop_words(doc, stopwords)\n",
    "\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.helpers import getTopSortedKElements, saveWordsToFile, getStopwordsFromFile\n",
    "stopwords = getStopwordsFromFile('stopwords_idfvalues.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open('../Data/arl', 'rb')\n",
    "articles = pickle.load(pfile)\n",
    "pfile.close()\n",
    "\n",
    "\n",
    "txt1 = articles[0]\n",
    "sentences = sent_tokenize(txt1)\n",
    "from summarize.summary import build_summary_using_tfidf\n",
    "from lib.summariza_word_frequency import summariza_word_frequency\n",
    "from lib.summariza_tfidf import summariza_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected sentences: [9, 8, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Abashakashatsi bagaragaje ko agakoko gatera igituntu kugira ngo kabeho, gakenera intungamubiri zizwi nk’ubutare (fer). Markus Seeger, umwarimu mu ishami ry’ubuvuzi muri Kaminuza ya Zurich (UZH), akaba no mu itsinda ryakoze ubushakashatsi yavuze ko basanze ako gakoko gatera igituntu iyo kageze mu mubiri gacura utundi turemangingo intungamubiri z’ubutare. Seeger yavuze ko babashije guhagarika utunyangingo duto twitwa IrtAB dutwara ubutare tubujyana mu gakoko gatera igituntu, kadashobora gukura cyangwa kakaba kanapfa, bityo igituntu ntikizahaze abantu.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build_summary_using_wordsFreq(txt1, stopwords, 3)\n",
    "summariza_word_frequency(txt1, 3, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen sentences: [4, 6, 10]\n",
      "Ibi byagaragaje uburyo bushya bushobora kwifashishwa n’abahanga mu gukora imiti y’igituntu no kugihashya kitarangiza abantu.\n",
      "Igituntu giterwa n’agakoko kazwi nka Mycobacterium tuberculosis, kandi kaboneka kenshi mu mubiri w’umuntu.\n",
      "Ishami ry’Umuryango w’Abibumbye ryita ku Buzima (OMS), ryatangaje ko umwaka ushize abantu miliyoni 1.5 bishwe n’indwara zibasira ibihaha n’igituntu kirimo.\n"
     ]
    }
   ],
   "source": [
    "build_summary_using_tfidf(txt1, stopwords, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary:\n",
      "Ibi byagaragaje uburyo bushya bushobora kwifashishwa n’abahanga mu gukora imiti y’igituntu no kugihashya kitarangiza abantu. Igituntu giterwa n’agakoko kazwi nka Mycobacterium tuberculosis, kandi kaboneka kenshi mu mubiri w’umuntu. Seeger yavuze ko babashije guhagarika utunyangingo duto twitwa IrtAB dutwara ubutare tubujyana mu gakoko gatera igituntu, kadashobora gukura cyangwa kakaba kanapfa, bityo igituntu ntikizahaze abantu.\n",
      "selected sentences: f[4, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "summariza_tfidf(txt1, 3, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "norm_text = normalize_text(txt1, stopwords_removal=True, stopwords=stopwords)\n",
    "tokens = word_tokenize(norm_text)\n",
    "sentences = sent_tokenize(txt1)\n",
    "wordsFreq = FreqDist(tokens)\n",
    "def score_sentence(sentence, cdict, stopwords):\n",
    "    sentence_score = 0\n",
    "    clean_sentences = normalize_text(\n",
    "        sentence, stopwords_removal=True, stopwords=stopwords)\n",
    "    for word in word_tokenize(clean_sentences):\n",
    "        if word.lower() not in stopwords and word not in stopwords and len(word) > 1:\n",
    "            sentence_score += cdict[word.lower()]\n",
    "    return sentence_score\n",
    "score = score_sentence(sentences[-2], wordsFreq, stopwords)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "dfc = pd.DataFrame(word_count_vector.todense().tolist(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.T.to_dict('dict')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(text, stopwords=None):\n",
    "    # save original sentences in a list\n",
    "    original_sentences = sent_tokenize(text)\n",
    "    # preprocess the document\n",
    "    doc = normalize_text(text, stopwords_removal=True, stopwords=stopwords)\n",
    "    # get clean sentence tokens\n",
    "    clean_sentence_tokens = sent_tokenize(doc)\n",
    "    cv = CountVectorizer(tokenizer=word_tokenize)\n",
    "    # get word counts for the word in the document\n",
    "    word_count_vector = cv.fit_transform(clean_sentence_tokens)\n",
    "#     print(f'type of word_count_vector {type(word_count_vector)}')\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    \n",
    "    # count matrix\n",
    "    count_vector = cv.transform(clean_sentence_tokens)\n",
    "    \n",
    "    #tf-idf scores\n",
    "    tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
    "    \n",
    "    # feature names\n",
    "    feature_names = cv.get_feature_names()\n",
    "    \n",
    "#     df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "    # sort ascending\n",
    "#     df_idf.sort_values(by=['idf_weights'])\n",
    "    df = pd.DataFrame(tf_idf_vector.todense().tolist(), columns=feature_names)\n",
    "    \n",
    "    return df.T.to_dict('dict')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = getTFIDF(txt1, stopwords)\n",
    "def score_sentence(sentence, tfidf_dict, stopwords):\n",
    "    sentence_score = 0\n",
    "    clean_sentence = normalize_text(sentence, stopwords_removal=True, stopwords=stopwords)\n",
    "    for word in word_tokenize(clean_sentence):\n",
    "        if word.lower() not in stopwords and word not in stopwords and len(word) > 1:\n",
    "            sentence_score = sentence_score + tfidf_dict[word.lower()]\n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "# from summarize.score import score_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentence_scores[i] = score_sentence(sentence, dff, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences_indexes = []\n",
    "for n in range(0, 2):\n",
    "    top_sentences_indexes.append(sorted_sentences[n][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in top_sentences_indexes:\n",
    "    print(sentences[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from summarize.tfidf import getTFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarize.summary import build_summary_using_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_summary_using_tfidf(txt1, stopwords, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_summary_using_tfidf(txt1, stopwords, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
